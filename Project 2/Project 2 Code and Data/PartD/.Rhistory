my_data <- Default %>%
mutate(id = row_number())
set.seed(123)
train <- my_data %>%
sample_frac(0.70)
test <- my_data %>% # testsettet er de resterende 30% som vi finner ved funksjonen anti_join
anti_join(train, by = "id")
model1 <- glm(default ~ balance, data = train,
family = "binomial")
summary(model1)
exp(coef(model1))
to_personer <- data.frame(balance = c(1000, 2000))
pred <- predict(model1, newdata = to_personer, type = "response")
pred
ifelse(pred > 0.5, "Yes", "No")
# Oppgave 2.4
model2 <- glm(default ~ student, data = train,
family = "binomial")
summary(model2)
exp(coef(model2))
to_folk <- data.frame(student = c("Yes", "No"))
pred1 <- predict(model2, newdata = to_folk, type = "response")
#Sannsynligheten for at en student og en ikkestudent misligholder gjelden sin: 1(student), 2(ikke-student)
pred1
model3 <- glm(default ~ student + balance + income, data = train,
family = "binomial")
summary(model3)
exp(coef(model3))
boxplot(balance ~ student, data = Default,
ylab = "balance", xlab = "default")
to_mennesker <- data.frame(student = c("Yes", "No"), balance = 1500,
income = 10000)
pred2 <- predict(model3, newdata = to_mennesker, type = "response")
pred2
ifelse(pred2 > 0.5, "Yes", "No")
library(caret)
# R-kode dersom vi vil velge k automatisk
set.seed(200)
trControl <- trainControl(method = "cv", # 5-fold kryssvalidering
number = 5)
# Tilpasser modellen
model4 <- train(default ~ balance + income + student,
data = train,
method = "knn",
trControl = trControl,
metric = "Accuracy")
# Hvilken k valgte kryssvalideringen?
k <- model4$finalModel$k
k
to_kunder <- data.frame(balance = c(1000, 2000), income = 10000, student = "Yes")
predict(model4, newdata = to_kunder)
sann <- test$default # Den sanne verdien av default i testdataene
sann
pred_logreg <- predict(model3, newdata = test, type = "response") # Predikert sannsynlighet
klass_logreg <- ifelse(pred_logreg > 0.5, "Yes", "No") # Klassifisering av kundene
logreg_tab <- table(sann, klass_logreg) # Kontigenstabell
logreg_tab
logreg_tab_norm <- logreg_tab %>%
prop.table %>% # normaliser
round(3) # rund av til 3 desimaler
logreg_tab_norm
logreg_tot <- sum(diag(logreg_tab_norm)) # Total andel korrekt klassfisering
logreg_tot
pred_logreg2 <- predict(model4, newdata = test, type = "response") # Predikert sannsynlighet
pred_logreg2 <- predict(model4, newdata = test, type = "prob") # Predikert sannsynlighet
klass_logreg2 <- ifelse(pred_logreg2 > 0.5, "Yes", "No") # Klassifisering av kundene
logreg_tab2 <- table(sann, klass_logreg2) # Kontigenstabell
View(klass_logreg2)
View(klass_logreg2)
pred_logreg2 <- predict(model4, newdata = test, type = "raw") # Predikert sannsynlighet
klass_logreg2 <- ifelse(pred_logreg2 > 0.5, "Yes", "No") # Klassifisering av kundene
pred_logreg2 <- predict(model4, newdata = test, type = "prob") # Predikert sannsynlighet
View(pred_logreg2)
pred_logreg2 <- predict(model4, newdata = test) # Predikert sannsynlighet
klass_logreg2 <- ifelse(pred_logreg2 > 0.5, "Yes", "No") # Klassifisering av kundene
logreg_tab2 <- table(sann, pred_logreg2) # Kontigenstabell
logreg_tab2
pred_logreg2 <- predict(model4, newdata = test) # Predikert sannsynlighet
logreg_tab2 <- table(sann, pred_logreg2) # Kontigenstabell
logreg_tab2
logreg_tab_norm2 <- logreg_tab2 %>%
prop.table %>% # normaliser
round(3) # rund av til 3 desimaler
logreg_tab_norm2
logreg_tot2 <- sum(diag(logreg_tab_norm2)) # Total andel korrekt klassfisering
logreg_tot2
library(forecast)
dax <- EuStockMarkets[ ,1] #datasettet
plot(dax)
trening <- head(dax, length(dax) - 10)
test <- tail(dax, 10)
fit_exp <- HoltWinters(trening) # tilpasset modell ved hjelp av eksponentiell glatting
pred_exp <- forecast(fit_exp, h = 10)
b <- auto.arima(trening) #Tilpasset modell ved hjelp av arima
accuracy(pred_exp, test)
b <- auto.arima(trening) #Tilpasset modell ved hjelp av arima
summary(Arima_tilpassning)
summary(b)
b <- auto.arima(trening) #Tilpasset modell ved hjelp av arima
summary(b)
accuracy(b, test)
summary(b)
accuracy(b, test)
pred_expa <- forecast(b, h = 10)
accuracy(pred_expa, test)
accuracy(pred_exp, test)
plot(pred_expa)
plot(pred_expa, include = 100)
plot(pred_exp, include = 100) # plot av figuren
plot(pred_expa, include = 100)
# Load in required packages - you will need to install these using
# install.packages("packagename") where you replace name with the
# name of the package.
library(tidyverse)
library(magrittr)
library(readxl)
library(ggplot2)
library(lubridate)
library(zoo)
# Load the latest data from SSB.
# This gives us the current version of GDP growth - denoted Y.
# We also create the lags 1 and 2 of Y.
load("latest_data.Rdata")
# Load in required packages - you will need to install these using
# install.packages("packagename") where you replace name with the
# name of the package.
library(tidyverse)
library(magrittr)
library(readxl)
library(ggplot2)
library(lubridate)
library(zoo)
# Load the latest data from SSB.
# This gives us the current version of GDP growth - denoted Y.
# We also create the lags 1 and 2 of Y.
load("latest_data.Rdata")
df_latest_release <-
df_latest_release %>%
mutate(
lag_1_Y = lag(Y, n = 1, order_by = quarter),
lag_2_Y = lag(Y, n = 2, order_by = quarter))
# Load in required packages - you will need to install these using
# install.packages("packagename") where you replace name with the
# name of the package.
library(tidyverse)
library(magrittr)
library(readxl)
library(ggplot2)
library(lubridate)
library(zoo)
# Load the latest data from SSB.
# This gives us the current version of GDP growth - denoted Y.
# We also create the lags 1 and 2 of Y.
load("latest_data.Rdata")
df_latest_release <-
df_latest_release %>%
mutate(
lag_1_Y = lag(Y, n = 1, order_by = quarter),
lag_2_Y = lag(Y, n = 2, order_by = quarter))
rm(list=ls())
getwd()
source("Rcorde.r")
getwd()
ls()
return(as)
as
as
as
SimAS<- function(S0, K, r, sigma, T, n){
t <- seq(1/252,T, by = 1/252)
h <- 1/252
m <- length(t)
e <- matrix(exp((r-0.5*sigma^2)*h+sigma*sqrt(h)*rnorm(n*m)),m,n)
S <- apply(e, 2, cumprod)
S <- S*S0
A <- colMeans(S)
C <- pmax(A-K,0)*exp(-r*T)
return(mean(C))
}
as <- simAS(100, 100, 0.05, 0.25, 1, 10000)
as <- simAS(100, 100, 0.05, 0.25, 1, 10000)
simAS <- function(S0, K, r, sigma, T, n){
t <- seq(1/252,T, by = 1/252)
h <- 1/252
m <- length(t)
e <- matrix(exp((r-0.5*sigma^2)*h+sigma*sqrt(h)*rnorm(n*m)),m,n)
S <- apply(e, 2, cumprod)
S <- S*S0
A <- colMeans(S)
C <- pmax(A-K,0)*exp(-r*T)
return(mean(C))
}
as <- simAS(100, 100, 0.05, 0.25, 1, 10000)
as
as <- simAS(100, 100, 0.05, 0.25, 1, 100000)
as
simAS <- function(S0, K, r, sigma, T, n){
t <- seq(1/252,T, by = 1/252)
h <- 1/252
m <- length(t)
e <- matrix(exp((r-0.5*sigma^2)*h+sigma*sqrt(h)*rnorm(n*m)),m,n)
S <- apply(e, 2, cumprod)
S <- S*S0
A <- colMeans(S)
C <- pmax(A-K,0)*exp(-r*T)
return(mean(C))
}
as <- simAS(100, 100, 0.05, 0.25, 1, 100000)
as
library(haven)
library(readr)
library(readxl)
library(tabulizer)
library(reshape2)
library(stringr)
library(stargazer)
library(ggplot2)
library(ggthemes)
library(RcppRoll)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(xts)
library(zoo)
library(ggrepel)
library(dplyr)
library(tree)
library(lattice)
library(randomForest)
library(caret)
library(e1071)
library(rpart)
library(rpart.plot)
library(extrafont)
library(plm)
install.packages("installr")
library(installr)
updateR()
install.packages("installr")
install.packages("installr")
library(installr)
updateR()
rm(list = ls())
test1 <- (L-(1-p)*D)/(p*L)-1
p <- 0.6
D <- 0.8
L <- 1.5
test1 <- (L-(1-p)*D)/(p*L)-1
test1
test2 <- ((1-p)*(L-D))/(p*L)
test2
test3 <- (p*U-I+L)/p*L-1
test3
p <- 0.6
D <- 0.8
L <- 1.5
U <- 3
I <- 2
test1 <- (L-(1-p)*D)/(p*L)-1
test1
test2 <- ((1-p)*(L-D))/(p*L)
test2
test3 <- (p*U-I+L)/p*L-1
test3
test4 <- (p*U+(1-p)*L-I)/p*L
test4
test4 <- (p*U+(1-p)*L-I)/(p*L)
test4
test3 <- (p*U-I+L)/(p*L)-1
test3
test4 <- (p*U+(1-p)*L-I)/(p*L)
test4
data(affairs, package="wooldridge")
install.packages("wooldrige")
data(affairs, package="wooldridge")
library(tidyverse)
library(tidymodels)
install.packages("tidymodels")
library(readxl)
library(xgboost)
install.packages("xgboost")
# Renser miljÃ¸et
rm(list = ls())
swirl()
library(swirl)
swirl()
Sys.Date()
mean(c(2, 4, 5))
submit()
boring_function('My first function!')
boring_function
submit()
my_mean(c(4,5,10))
submit()
remainder(5)
remainder(11,5)
remainder(divisor = 11, num = 5)
remainder(4, div = 2)
args(remainder)
submit()
submit()
submit()
evaluate(sd, c(1.4, 3.6, 7.9,
| 8.8))
evaluate(sd, c(1.4, 3.6, 7.9,8.8))
evaluate(function(x){x+1}, 6)
evaluate(function(x){x[1]}, c(8, 4, 0))
evaluate(function(x){tail(x, 1)}, c(8, 4, 0))
paste
?paste
("Programming", "is", "fun!")
paste("Programming", "is", "fun!")
submit()
submit()
submit()
telegram(hi there)
telegram("hi there")
submit()
mad_libs("New York, brilliant, building")
mad_libs("New York brilliant building")
mad_libs("New York", "brilliant", "building")
submit()
I %p% love %p% R!
"I" %p% "love" %p% "R!"
# Clear environment
rm(list = ls())
# Libraries
library(readxl)
# Clear environment
rm(list = ls())
#Load libraries
library(sentimentr)
library(tidyverse)
#Load data
firm <- load("../data/firm_dataset.Rdata")
setwd("~/8.Hostsemester 2022 (5.kull)/BAN432/Individual Assignments/Scripts")
#Load data
firm <- load("../data/firm_dataset.Rdata")
View(raw.data)
# Finding the indices for the selected industry
indices <- which(df$industry.fama.french.49 == "46 Insur")
# Clear environment
rm(list = ls())
#Load libraries
library(sentimentr)
library(tidyverse)
#Load data
load("../data/firm_dataset.Rdata")
# Finding the indices for the selected industry
indices <- which(raw.data$industry.fama.french.49 == "46 Insur")
section.7.mda[indices]
insurance <-section.7.mda[indices]
others <- section.7.mda[1:500 %in% !indices]
# Finding the indices for the selected industry
indices_ins <- which(raw.data$industry.fama.french.49 == "46 Insur")
indices_other <- which(raw.data$industry.fama.french.49 != "46 Insur")
insurance <-section.7.mda[indices_ins]
others <- section.7.mda[indices_other]
extract_sentiment_terms(insurance[1], polarity_dt = hash_sentiment_jockers_rinker)
library(lexicon)
extract_sentiment_terms(insurance[1], polarity_dt = hash_sentiment_jockers_rinker)
sentiment(insurance[1])
sentiment_by(insurance[1])
sentiment_by(insurance)
sentiment_ins <- sentiment_by(insurance)
sintiment_others <- sentiment_by(others)
# Average sentiment insurance:
sentiment_ins$ave_sentiment/length(sentiment_ins)
View(sentiment_ins)
# Average sentiment insurance:
mean(sentiment_ins)
View(sentiment_ins)
# Average sentiment insurance:
mean(sentiment_ins$ave_sentiment)
mean(sintiment_others$ave_sentiment)
extract_sentiment_terms(insurance[1], polarity_dt = hash_sentiment_jockers_rinker)
View(sentiment_ins)
d <- extract_sentiment_terms(insurance[1], polarity_dt = hash_sentiment_jockers_rinker)
View(d)
d %>%
unnest(positive)
d %>%
unnest(positive, negative)
d %>%
unnest(negative)
d %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n())
d %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n()) %>%
arrange(-freq)
d %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n()/sum(freq)) %>%
arrange(-freq)
d %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n())
d %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n()) %>%
mutate(freq = freq/sum(freq))
d %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n()) %>%
mutate(freq = freq/sum(freq))
arrange(-freq)
d %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n()) %>%
mutate(freq = freq/sum(freq))
arrange(-)
arrange(-freq)
d %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n()) %>%
mutate(freq = freq/sum(freq)) %>%
arrange(-freq)
# Extracting and finding the frequency
freq_ins <- extract_sentiment_terms(insurance, polarity_dt = hash_sentiment_jockers_rinker)
freq_ins %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n()) %>%
mutate(freq = freq/sum(freq)) %>%
arrange(-freq)
# Clear environment
rm(list = ls())
#Load libraries
library(sentimentr)
library(tidyverse)
library(lexicon)
#Load data
load("../data/firm_dataset.Rdata")
# Finding the indices for the selected industry
indices_ins <- which(raw.data$industry.fama.french.49 == "46 Insur")
indices_other <- which(raw.data$industry.fama.french.49 != "46 Insur")
insurance <-section.7.mda[indices_ins]
others <- section.7.mda[indices_other]
sentiment_ins <- sentiment_by(insurance)
sentiment_others <- sentiment_by(others)
# Average sentiment insurance:
mean(sentiment_ins$ave_sentiment)
mean(sentiment_others$ave_sentiment)
# We can see that the average sentiment is greater for others than for the insurance companies.
extract_sentiment_terms(insurance, polarity_dt = hash_sentiment_jockers_rinker)
# Extracting and finding the frequency
freq_ins <- extract_sentiment_terms(insurance, polarity_dt = hash_sentiment_jockers_rinker)
freq_ins %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n()) %>%
mutate(freq = freq/sum(freq)) %>%
arrange(-freq)
freq_others <- extract_sentiment_terms(others, polarity_dt = hash_sentiment_jockers_rinker)
freq_others %>%
unnest(negative) %>%
group_by(negative) %>%
summarise(freq = n()) %>%
mutate(freq = freq/sum(freq)) %>%
arrange(-freq)
setwd("~/8.Hostsemester 2022 (5.kull)/BAN402/Project2/ban402-project2/PartD")
# Clear environment
rm(list = ls())
# Load libraries
library(readxl)
library(tidyverse)
# Load data
df <- read_excel("Proj2D.xlsx")
# Clear environment
rm(list = ls())
# Load libraries
library(readxl)
library(tidyverse)
# Load data
df <- read_excel("Proj2D.xlsx")
# Finding weekdays for each day and
df$month_number <- match(df$Month, month.name)
df$date <- paste0(df$Day,"/",df$month_number,"/",2022)
df$date1 <- as.Date(df$date, format = "%d/%m/%Y")
df$weekday <- weekdays.Date(df$date1)
# Finding the number of days:
ndays <- df %>%
count(date) %>%
summarise(n = n())
ndays <- ndays$n
nhours <- 24
# Adjusting our datafram and selecting the columns we need:
final <- df %>%
mutate(KwhPrice = `Price (EUR/MWh)`/1000) %>%
select(1,2,8,9,3)
# Creating a price matrix
price_matrix <- matrix(0, nrow = nhours, ncol = ndays)
price_indices <- 1:nrow(final)
price_matrix[price_indices] <- final$KwhPrice
price_matrix <- as.data.frame(t(price_matrix))
colnames(price_matrix) <- 1:nhours
# write.csv(price_matrix,"price_matrix.csv", row.names = FALSE)
# Creating a new index for the hours:
hours <- final$Hour[1:24]
hourindex <- 1:24
final$Hour[final$Hour %in% hours] <- hourindex
View(final)
